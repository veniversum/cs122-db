CS122 Assignment 3 - Table Statistics and Plan Costing - Design Document
========================================================================

A:  Logistics
-------------

A1.  List your team name and the people who worked on this assignment.

     <team name>'); DROP TABLE Teams;--

     Qingzhuo Aw Young <qingzhuo@caltech.edu>
     Timur Kuzhagaliyev <timbokz@caltech.edu>

A2.  Specify the repository URL, tag name and commit-hash of the Git version
     you are submitting for your assignment.  (You can list the commit hashes
     of your repository tags with this command:  git show-ref --tags)

     Repository URL:  git@github.com:veniversum/cs122-db.git
     Tag name:        v3.0
     Commit hash:     c3d0914ac068298e0222e95e70e9723ef2f6a325

A3.  Specify any late tokens you are applying to this assignment, or
     "none" if no late tokens.

     None

A4.  Briefly describe what parts of the assignment each teammate focused on.

B:  Statistics Collection
-------------------------

B1.  Using pseudocode, summarize the implementation of your HeapTupleFile
     analyze() function.

     initially:
         page <- page 0
         numDataPages <- 0
         totalTupleSize <- 0
         numTuples <- 0
         collectors <- list of column stat collectors

     while not end of file:
         page <- next page in db file
         numDataPages <- numDataPages + 1
         totalTupleSize <- totalTupleSize + pageDataEnd - pageDataStart
         for slot in page if slot is not empty:
             numTuples <- numTuples + 1
             for column in slot:
                 add column value to corresponding collector

     avgTupleSize <- totalTupleSize / numTuples
     generate stats from (numDataPages, numTuples, avgTupleSize, collectors)
     save stats to header page as metadata


C:  Plan Costing Implementation
-------------------------------

C1.  Briefly describe how you estimate the number of tuples and the cost
     of a file-scan plan node.  What factors does your cost include?

     numTuples: selectivity * number of tuples in table stats.
                SelectivityEstimator is modified to work with
                null predicates.

     tupleSize: get directly from table stats

     cpuCost: same as number of tuples in table

     numBlockIOs: get number of data pages from table stats

     Cost of predicate is not included in cpu cost (for all plan nodes really).


C2.  Same question as for C1, but for simple filter nodes.

     numTuples: selectivity of predicate * number of tuples in child table.
                Selectivity is calculated using child schema and column stats.

     tupleSize: same as child table

     cpuCost: number of tuples in child table + CPU cost of child table

     numBlockIOs: same as child table.


C3.  Same question as for C1, but for nested-loop joins.

     Let N be number of tuples in left child node (outer loop)
     and M be the number of tuples in right child nodes (inner loop).

     numTuples: selectivity of predicate * N*M. If OUTER join, add
                (1 - selectivity) * number of tuples in outer table.

                Selectivity is calculated using both children schema and column stats.
                Cross joins have selectivity of 1.

     tupleSize: sum of tuple size in child tables. We do not yet support
                joins that return less columns yet (like natural joins).
                We also do not handle edge cases like LEFT OUTER joins
                with only null values for the right table's columns.

     cpuCost: cpuCost of left child table + N * CPU cost of right child table.
              Additional CPU cost for each tuple produced in joins, since joined
              tuples cost more than skipped tuples.

     numBlockIOs: numBlockIOs of left child table + N * numBlockIOs of right child table.


D:  Costing SQL Queries
-----------------------

Answer these questions after you have loaded the stores-28K.sql data, and
have analyzed all of the tables in that schema.

D1.  Paste the output of running:  EXPLAIN SELECT * FROM cities;
     Do not include debug lines, just the output of the command itself.

        Explain Plan:
            FileScan[table:  CITIES] cost=[tuples=254.0, tupSize=23.8, cpuCost=254.0, blockIOs=1]

        Estimated 254.000000 tuples with average size 23.787401
        Estimated number of block IOs:  1


D2.  What is the estimated number of tuples that will be produced by each
     of these queries:

     SELECT * FROM cities WHERE population > 1000000;

        Explain Plan:
            FileScan[table:  CITIES, pred:  CITIES.POPULATION > 1000000] cost=[tuples=225.6, tupSize=23.8, cpuCost=254.0, blockIOs=1]

        Estimated 225.582245 tuples with average size 23.787401
        Estimated number of block IOs:  1


     SELECT * FROM cities WHERE population > 5000000;

        Explain Plan:
            FileScan[table:  CITIES, pred:  CITIES.POPULATION > 5000000] cost=[tuples=99.3, tupSize=23.8, cpuCost=254.0, blockIOs=1]

        Estimated 99.262199 tuples with average size 23.787401
        Estimated number of block IOs:  1


     SELECT * FROM cities WHERE population > 8000000;

        Explain Plan:
            FileScan[table:  CITIES, pred:  CITIES.POPULATION > 8000000] cost=[tuples=4.5, tupSize=23.8, cpuCost=254.0, blockIOs=1]

        Estimated 4.522162 tuples with average size 23.787401
        Estimated number of block IOs:  1


     How many tuples does each query produce?

        1st query: 226 estimated, 9 actual.
        2nd query: 99 estimated, 1 actual.
        3rd query: 5 estimated, 1 actual.


     Briefly explain the difference between the estimated number of tuples
     and the actual number of tuples for these queries.

        In our calculations, we're assuming uniform distribution of tuples with
        column values in range [min, max] from column stats. The actual
        distribution of population values in `cities` table is not uniform,
        which is why are our estimates for predicate values far away from the
        min/max are inaccurate.


D3.  Paste the output of running these commands:

     EXPLAIN SELECT store_id FROM stores, cities
     WHERE stores.city_id = cities.city_id AND
           cities.population > 1000000;

        Explain Plan:
            Project[values:  [STORES.STORE_ID]] cost=[tuples=1776.2, tupSize=36.8, cpuCost=2035776.3, blockIOs=2004]
                SimpleFilter[pred:  STORES.CITY_ID == CITIES.CITY_ID AND CITIES.POPULATION > 1000000] cost=[tuples=1776.2, tupSize=36.8, cpuCost=2034000.0, blockIOs=2004]
                    NestedLoop[join type: CROSS, no pred] cost=[tuples=508000.0, tupSize=36.8, cpuCost=1526000.0, blockIOs=2004]
                        FileScan[table:  STORES] cost=[tuples=2000.0, tupSize=13.0, cpuCost=2000.0, blockIOs=4]
                        FileScan[table:  CITIES] cost=[tuples=254.0, tupSize=23.8, cpuCost=254.0, blockIOs=1]

        Estimated 1776.238159 tuples with average size 36.787399
        Estimated number of block IOs:  2004


     EXPLAIN SELECT store_id FROM stores JOIN
                    (SELECT city_id FROM cities
                     WHERE population > 1000000) AS big_cities
                    ON stores.city_id = big_cities.city_id;

        Explain Plan:
            Project[values:  [STORES.STORE_ID]] cost=[tuples=1776.0, tupSize=36.8, cpuCost=1415881.0, blockIOs=2004]
                NestedLoop[join type: INNER, pred:  STORES.CITY_ID == BIG_CITIES.CITY_ID] cost=[tuples=1776.0, tupSize=36.8, cpuCost=1414105.0, blockIOs=2004]
                    FileScan[table:  STORES] cost=[tuples=2000.0, tupSize=13.0, cpuCost=2000.0, blockIOs=4]
                    Rename[resultTableName=BIG_CITIES] cost=[tuples=225.6, tupSize=23.8, cpuCost=479.6, blockIOs=1]
                        Project[values:  [CITIES.CITY_ID]] cost=[tuples=225.6, tupSize=23.8, cpuCost=479.6, blockIOs=1]
                            FileScan[table:  CITIES, pred:  CITIES.POPULATION > 1000000] cost=[tuples=225.6, tupSize=23.8, cpuCost=254.0, blockIOs=1]

        Estimated 1776.000000 tuples with average size 36.787399
        Estimated number of block IOs:  2004


     The estimated number of tuples produced should be the same, but the
     costs should be different.  Explain why.

        In the first query, we cross join all tuples from the 2 tables creating
        500k tuples. Both predicates from the query have to be evaluated on
        every single tuple, which makes the CPU cost skyrocket.

        In the first query, first we only consider a subset of tuples from the
        `cities` table and then we perform a INNER join (as opposed to CROSS)
        which creates less tuples by several orders of magnitude, hence why CPU
        cost is much lower.

D4.  The assignment gives this example "slow" query:

     SELECT store_id, property_costs
     FROM stores, cities, states
     WHERE stores.city_id = cities.city_id AND
           cities.state_id = states.state_id AND
           state_name = 'Oregon' AND property_costs > 500000;

     How long does this query take to run, in seconds?

        The query took 87 seconds to evaluate.

     Include the EXPLAIN output for the above query here.

        Explain Plan:
            Project[values:  [STORES.STORE_ID, STORES.PROPERTY_COSTS]] cost=[tuples=19.6, tupSize=52.5, cpuCost=105158016.0, blockIOs=510004]
                SimpleFilter[pred:  STORES.CITY_ID == CITIES.CITY_ID AND CITIES.STATE_ID == STATES.STATE_ID AND STATES.STATE_NAME == 'Oregon' AND STORES.PROPERTY_COSTS > 500000] cost=[tuples=19.6, tupSize=52.5, cpuCost=105158000.0, blockIOs=510004]
                    NestedLoop[join type: CROSS, no pred] cost=[tuples=25908000.0, tupSize=52.5, cpuCost=79250000.0, blockIOs=510004]
                        NestedLoop[join type: CROSS, no pred] cost=[tuples=508000.0, tupSize=36.8, cpuCost=1526000.0, blockIOs=2004]
                            FileScan[table:  STORES] cost=[tuples=2000.0, tupSize=13.0, cpuCost=2000.0, blockIOs=4]
                            FileScan[table:  CITIES] cost=[tuples=254.0, tupSize=23.8, cpuCost=254.0, blockIOs=1]
                        FileScan[table:  STATES] cost=[tuples=51.0, tupSize=15.7, cpuCost=51.0, blockIOs=1]

        Estimated 19.588217 tuples with average size 52.454067
        Estimated number of block IOs:  510004

     How would you rewrite this query (e.g. using ON clauses, subqueries
     in the FROM clause, etc.) to be as optimal as possible?  Also include
     the result of EXPLAINing your query.

        The general strategy is to filter out as many unwanted tuples as
        possible before performing joins. The resultant query can be seen below
        and takes 0.6597 seconds to run.

        SELECT s.store_id, s.property_costs
        FROM
        (SELECT store_id, city_id, property_costs FROM stores WHERE property_costs > 500000) s
        JOIN (
            SELECT city_id
            FROM (SELECT state_id FROM states WHERE state_name = 'Oregon') st
            JOIN cities ci
            ON ci.state_id = st.state_id
        ) c
        ON s.city_id = c.city_id;

        Explain Plan:
            Project[values:  [S.STORE_ID, S.PROPERTY_COSTS]] cost=[tuples=15.0, tupSize=52.5, cpuCost=574456.4, blockIOs=2001]
                NestedLoop[join type: INNER, pred:  S.CITY_ID == C.CITY_ID] cost=[tuples=15.0, tupSize=52.5, cpuCost=574441.4, blockIOs=2001]
                    Rename[resultTableName=S] cost=[tuples=999.0, tupSize=13.0, cpuCost=2999.0, blockIOs=4]
                        Project[values:  [STORES.STORE_ID, STORES.CITY_ID, STORES.PROPERTY_COSTS]] cost=[tuples=999.0, tupSize=13.0, cpuCost=2999.0, blockIOs=4]
                            FileScan[table:  STORES, pred:  STORES.PROPERTY_COSTS > 500000] cost=[tuples=999.0, tupSize=13.0, cpuCost=2000.0, blockIOs=4]
                    Rename[resultTableName=C] cost=[tuples=4.0, tupSize=39.5, cpuCost=568.0, blockIOs=2]
                        Project[values:  [CI.CITY_ID]] cost=[tuples=4.0, tupSize=39.5, cpuCost=568.0, blockIOs=2]
                            NestedLoop[join type: INNER, pred:  CI.STATE_ID == ST.STATE_ID] cost=[tuples=4.0, tupSize=39.5, cpuCost=564.0, blockIOs=2]
                                Rename[resultTableName=ST] cost=[tuples=1.0, tupSize=15.7, cpuCost=52.0, blockIOs=1]
                                    Project[values:  [STATES.STATE_ID]] cost=[tuples=1.0, tupSize=15.7, cpuCost=52.0, blockIOs=1]
                                        FileScan[table:  STATES, pred:  STATES.STATE_NAME == 'Oregon'] cost=[tuples=1.0, tupSize=15.7, cpuCost=51.0, blockIOs=1]
                                Rename[resultTableName=CI] cost=[tuples=254.0, tupSize=23.8, cpuCost=254.0, blockIOs=1]
                                    FileScan[table:  CITIES] cost=[tuples=254.0, tupSize=23.8, cpuCost=254.0, blockIOs=1]

        Estimated 15.000000 tuples with average size 52.454067
        Estimated number of block IOs:  2001


E:  Extra Credit [OPTIONAL]
---------------------------

If you implemented any extra-credit tasks for this assignment, describe
them here.  The description should be like this, with stuff in "<>" replaced.
(The value i starts at 1 and increments...)

E<0>: I think first part of extra credit COLUMN op VALUE statistics was already
      supported? Or the design accidentally supported it.

E<1>:  Selectivity for `col IN (values)`

     Extended SelectivityEstimator.estimateSelectivity() to calculate the
     selectivity of the InValuesOperator.

     `TestInValuesOperatorSelectivity` test file:
        - Checks that selectivity is calculated correctly on tables with uniform
          distribution of values.
        - Checks that selectivity is calculated correctly on varchar column with
          no null values.
        - Checks that selectivity is calculated correctly on varchar column with
          null values.

E<2>:  More accurate selectiv. for `col == val1 OR col == val2 OR ...`

     Extended SelectivityEstimator.estimateSelectivity() to recognise when when
     OR boolean operator consists of terms that check if the same column is
     equal to some values. If so, treat them as InValuesOperator.

     `TestOrBooleanSameColSelectivity` test file:
        - Tests same things as the InValuesOperator selectivity test file, but
          uses a BooleanOperator to trigger the calculation

E<3>:  Tests for table analysis

     `TestTableAnalysis` test file:
        - Tests that table analysis was performed correctly.

E<4>: Tests for ORDER BY clause, to check order.

    `TestGroupingAndAggregation#testWithOtherCommands`
        - Perform check correctly without ignoring order

E<5>: Fix bug in TupleComparator / tests with floating points

    See https://piazza.com/class/jc8a2ohck6y5px?cid=35.

F:  Feedback [OPTIONAL]
-----------------------

WE NEED YOUR FEEDBACK!  Thoughtful and constructive input will help us to
improve future versions of the course.  These questions are OPTIONAL, and
they obviously won't affect your grade in any way (including if you hate
everything about the assignment and databases in general, or Donnie and/or
the TAs in particular).  Feel free to answer as many or as few of them as
you wish.

NOTE:  If you wish to give anonymous feedback, a similar survey will be
       made available on the Moodle.

F1.  How many hours total did your team spend on this assignment?
     (That is, the sum of each teammate's time spent on the assignment.)

F2.  What parts of the assignment were most time-consuming?  Why?

F3.  Did you find any parts of the assignment particularly instructive?
     Correspondingly, did any parts feel like unnecessary busy-work?

F4.  Did you particularly enjoy any parts of the assignment?  Were there
     any parts that you particularly disliked?

F5.  Do you have any suggestions for how future versions of the
     assignment can be improved?
